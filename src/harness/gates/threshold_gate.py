"""Gate threshold evaluation logic."""

from __future__ import annotations

import json
from collections.abc import Mapping
from pathlib import Path

from harness.core.gates import GateResult
from harness.loaders.policy_loader import PolicyLoadError, load_policy
from harness.utils.errors import HarnessError


class GateEvaluationError(HarnessError):
    """Error during gate evaluation."""


def load_thresholds_from_policy(policy_path: Path | str | None = None) -> dict[str, float]:
    """Load threshold values from a policy file.

    Extracts thresholds from the policy YAML file (e.g., harmful_output_rate: 0.05).

    Args:
        policy_path: Path to policy file (default: policies/content_policy.yaml)

    Returns:
        Dictionary mapping threshold names to values

    Raises:
        GateEvaluationError: If policy cannot be loaded or has no thresholds
    """
    if policy_path is None:
        policy_path = Path("policies/content_policy.yaml")
    else:
        policy_path = Path(policy_path)

    if not policy_path.exists():
        raise GateEvaluationError(f"Policy file not found: {policy_path}")

    try:
        # Load policy using existing policy loader
        load_policy(policy_path)

        # Extract thresholds from policy YAML
        # The policy loader doesn't currently extract thresholds, so we need to read the YAML directly
        import yaml

        with policy_path.open("r", encoding="utf-8") as f:
            policy_data = yaml.safe_load(f)

        thresholds: dict[str, float] = {}
        if "thresholds" in policy_data:
            thresholds = {k: float(v) for k, v in policy_data["thresholds"].items()}

        if not thresholds:
            # No thresholds defined - return empty dict (gates will pass by default)
            return {}

        return thresholds

    except PolicyLoadError as e:
        raise GateEvaluationError(f"Failed to load policy: {e}") from e
    except Exception as e:
        raise GateEvaluationError(f"Error loading thresholds from policy: {e}") from e


def load_metrics_from_summary(summary_path: Path | str) -> dict[str, float]:
    """Load metrics from a summary JSON file.

    Extracts all metrics from the summary.json file generated by JSONReporter.

    Args:
        summary_path: Path to summary.json file

    Returns:
        Dictionary mapping metric names to values

    Raises:
        GateEvaluationError: If summary file cannot be loaded or is invalid
    """
    summary_path = Path(summary_path)

    if not summary_path.exists():
        raise GateEvaluationError(f"Summary file not found: {summary_path}")

    try:
        with summary_path.open("r", encoding="utf-8") as f:
            data = json.load(f)

        # Extract metrics from summary
        metrics: dict[str, float] = {}

        # Core metrics
        if "total" in data:
            metrics["total"] = float(data["total"])
        if "passed" in data:
            metrics["passed"] = float(data["passed"])
        if "failed" in data:
            metrics["failed"] = float(data["failed"])

        # Rate metrics
        if "harmful_output_rate" in data:
            metrics["harmful_output_rate"] = float(data["harmful_output_rate"])
        if "tool_policy_violation_rate" in data:
            metrics["tool_policy_violation_rate"] = float(data["tool_policy_violation_rate"])
        if "utility_failure_rate" in data:
            metrics["utility_failure_rate"] = float(data["utility_failure_rate"])
        if "critical_violation_rate" in data:
            metrics["critical_violation_rate"] = float(data["critical_violation_rate"])

        # Latency and cost metrics
        if "latency_ms_p50" in data:
            metrics["latency_ms_p50"] = float(data["latency_ms_p50"])
        if "cost_usd" in data:
            metrics["cost_usd"] = float(data["cost_usd"])

        # Calculate critical_violation_rate if not present but we have detector results
        if "critical_violation_rate" not in metrics and "results" in data:
            critical_count = 0
            total = metrics.get("total", 0)
            for result in data.get("results", []):
                if "detector_results" in result:
                    for detector_result in result["detector_results"]:
                        for violation in detector_result.get("violations", []):
                            if violation.get("severity") == "critical":
                                critical_count += 1
                                break  # Count once per result
            if total > 0:
                metrics["critical_violation_rate"] = critical_count / total

        return metrics

    except json.JSONDecodeError as e:
        raise GateEvaluationError(f"Invalid JSON in summary file: {e}") from e
    except Exception as e:
        raise GateEvaluationError(f"Error loading metrics from summary: {e}") from e


def load_metrics_from_junit(junit_path: Path | str) -> dict[str, float]:
    """Parse JUnit XML and extract metrics for gate evaluation.
    
    Args:
        junit_path: Path to junit.xml file
        
    Returns:
        Metrics dict compatible with gate evaluation
        
    Raises:
        GateEvaluationError: If junit file cannot be loaded or is invalid
    """
    import xml.etree.ElementTree as ET
    
    junit_path = Path(junit_path)
    
    if not junit_path.exists():
        raise GateEvaluationError(f"JUnit file not found: {junit_path}")
    
    try:
        tree = ET.parse(junit_path)
        root = tree.getroot()
        
        # Extract from <testsuite> attributes
        testsuite = root.find("testsuite")
        if testsuite is None:
            testsuite = root  # Sometimes root is testsuite
        
        total = int(testsuite.get("tests", 0))
        failures = int(testsuite.get("failures", 0))
        errors = int(testsuite.get("errors", 0))
        skipped = int(testsuite.get("skipped", 0))
        
        passed = total - failures - errors - skipped
        
        metrics = {
            "total": float(total),
            "passed": float(passed),
            "failed": float(failures + errors),
            "skipped": float(skipped),
        }
        
        # Add success rate
        if total > 0:
            metrics["success_rate"] = passed / total
            metrics["pass_rate"] = passed / total
        
        return metrics
    
    except ET.ParseError as e:
        raise GateEvaluationError(f"Invalid XML in JUnit file: {e}") from e
    except Exception as e:
        raise GateEvaluationError(f"Error loading metrics from JUnit: {e}") from e


def evaluate_gates(
    metrics: Mapping[str, float],
    thresholds: dict[str, float],
    fail_on: list[str] | None = None,
    warn_on: list[str] | None = None,
) -> GateResult:
    """Evaluate metrics against threshold gates.

    Creates ThresholdGate instances for each threshold and evaluates them.
    For "rate" metrics (like harmful_output_rate), lower is better (metric <= threshold).
    For "success" metrics (like pass_rate), higher is better (metric >= threshold).

    Args:
        metrics: Dictionary of metric name to value
        thresholds: Dictionary of threshold name to maximum/minimum value
        fail_on: Optional list of conditions that cause failure (e.g., ["harmful_output_rate > 0"])
        warn_on: Optional list of conditions that cause warnings (not implemented yet)

    Returns:
        GateResult with overall pass/fail and detailed reasons
    """
    if not thresholds:
        # No thresholds defined - gate passes
        return GateResult(
            passed=True,
            reason="No thresholds defined - gate passes by default",
            metrics=dict(metrics),
        )

    gate_results: list[tuple[str, bool, str]] = []  # (name, passed, reason)
    all_passed = True

    # Evaluate each threshold
    for threshold_name, threshold_value in thresholds.items():
        metric_value = metrics.get(threshold_name, 0.0)

        # Determine if this is a "lower is better" or "higher is better" metric
        # Rate metrics (harmful_output_rate, violation_rate, failure_rate) are "lower is better"
        # Success metrics (pass_rate) would be "higher is better" but we don't have those yet
        is_rate_metric = any(
            threshold_name.endswith(suffix)
            for suffix in ["_rate", "_violation_rate", "_failure_rate", "_output_rate"]
        )

        if is_rate_metric:
            # For rate metrics: pass if metric <= threshold (lower is better)
            passed = metric_value <= threshold_value
            comparison = "<="
        else:
            # For other metrics: pass if metric >= threshold (higher is better)
            passed = metric_value >= threshold_value
            comparison = ">="

        reason = (
            f"{threshold_name}={metric_value:.4f} {comparison} {threshold_value:.4f} "
            f"({'PASS' if passed else 'FAIL'})"
        )
        gate_results.append((threshold_name, passed, reason))

        if not passed:
            all_passed = False

    # Check fail_on conditions
    if fail_on:
        for condition in fail_on:
            # Simple condition parsing: "metric > value" or "metric < value"
            if ">" in condition:
                parts = condition.split(">")
                if len(parts) == 2:
                    metric_name = parts[0].strip()
                    threshold_str = parts[1].strip()
                    try:
                        threshold_val = float(threshold_str)
                        metric_val = metrics.get(metric_name, 0.0)
                        if metric_val > threshold_val:
                            all_passed = False
                            gate_results.append(
                                (
                                    f"{metric_name} (fail_on)",
                                    False,
                                    f"{metric_name}={metric_val:.4f} > {threshold_val:.4f} (FAIL_ON)",
                                )
                            )
                    except ValueError:
                        pass  # Skip invalid conditions
            elif "<" in condition:
                parts = condition.split("<")
                if len(parts) == 2:
                    metric_name = parts[0].strip()
                    threshold_str = parts[1].strip()
                    try:
                        threshold_val = float(threshold_str)
                        metric_val = metrics.get(metric_name, 0.0)
                        if metric_val < threshold_val:
                            all_passed = False
                            gate_results.append(
                                (
                                    f"{metric_name} (fail_on)",
                                    False,
                                    f"{metric_name}={metric_val:.4f} < {threshold_val:.4f} (FAIL_ON)",
                                )
                            )
                    except ValueError:
                        pass  # Skip invalid conditions

    # Build reason string from all gate results
    reasons = [reason for _, _, reason in gate_results]
    reason_str = "; ".join(reasons)

    if not all_passed:
        reason_str = f"Gate failed: {reason_str}"
    else:
        reason_str = f"Gate passed: {reason_str}"

    return GateResult(
        passed=all_passed,
        reason=reason_str,
        metrics=dict(metrics),
    )
